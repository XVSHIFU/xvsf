---
title: "页面数据爬取"
date: 2026-01-28T12:00:00+08:00
categories:
  - "Python数据爬取与可视化"
---

# 简易模板
```python
import requests
from bs4 import BeautifulSoup

meHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

url = ""

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('utf-8')
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all()
```





## 单封家书【译文】内容获取


目标网站:[http://ewenyan.com/articles/zgfjs/1.html](http://ewenyan.com/articles/zgfjs/1.html)



<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927101.png)



### 待爬取数据分析
确认编码:

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927453.png)



因为此网站的文章数据全部放在`<p>`标签内，并且页面布局都是以`<table>`标签来布局的。

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291926384.png)



所以先把所有的`<table>`标签抓取下来，然后按对应顺序爬取文章。





可以确认爬取数据的位置为第四个`<table>`标签

```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312',errors='ignore')
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')
    print(t)

    indx = 0
    for i in t:
        print(i)
        print(indx)
        indx += 1


getOne(url)

```

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927949.png)





完善代码：

```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3]
    print(t)

getOne(url)

```

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927403.png)





### 提取文本
```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    print(t)

    

getOne(url)

```



<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927956.png)



### 完善代码-爬取单封家书的译文
```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    t = t.split('【译文】')[1].strip()
    return t

    

print(getOne(url))

```

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927473.png)



## 目录页


### 数据分析
[http://ewenyan.com/contents/more/zgfjs.html](http://ewenyan.com/contents/more/zgfjs.html)

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927588.png)



<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291928616.png)

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927811.png)









确定爬取的信息

```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

#url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    t = t.split('【译文】')[1].strip()
    return t
#print(getOne(url))

url = "http://ewenyan.com/contents/more/zgfjs.html"    
def getAll(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')
    print(t)
getAll(url)

```



<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291928698.png)





<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927117.png)



爬取所有`<a>`标签

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927984.png)



### 分析链接：
```python
../../articles/zgfjs/89.html
http://ewenyan.com/articles/zgfjs/1.html

我们要使用 'http://ewenyan.com/' 去替换 '../../'
```



替换链接

```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

#url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    t = t.split('【译文】')[1].strip()
    return t
#print(getOne(url))

url = "http://ewenyan.com/contents/more/zgfjs.html"    
def getAll(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[2]
    t = t.find_all('a')
    print(t)

    for i in t:
        title = i.text
        link = i.get('href').replace('../../', 'http://ewenyan.com/')
        print(title, link)
 
        
getAll(url)

```

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927729.png)





### 排序问题


使用数组列表将数据进行存放

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291926631.png)



解决排序问题

```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

#url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    t = t.split('【译文】')[1].strip()
    return t
#print(getOne(url))

url = "http://ewenyan.com/contents/more/zgfjs.html"    
def getAll(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[2]
    t = t.find_all('a')

    ls = []
    for i in t:
        title = i.text
        link = i.get('href').replace('../../', 'http://ewenyan.com/')
        #print(title, link)
        indx = int(title.split('.')[0])
        ls.append([indx, title, link])
    ls.sort()
    print(ls)
 
        
getAll(url)

```

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927204.png)









## 家书内容存入文件


### 问题一：编码
```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

#url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    t = t.split('【译文】')[1].strip()
    return t
#print(getOne(url))

url = "http://ewenyan.com/contents/more/zgfjs.html"    
def getAll(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312', )
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[2]
    t = t.find_all('a')

    ls = []
    for i in t:
        title = i.text
        link = i.get('href').replace('../../', 'http://ewenyan.com/')
        #print(title, link)
        indx = int(title.split('.')[0])
        ls.append([indx, title, link])
    ls.sort()
    return ls 
        
lst = getAll(url)
for each in lst:
    f = open('曾国藩家书.txt', 'a')
    title = each[1]
    print(title)
    f.write(title)
    f.write(getOne(each[2]))
    f.close()

```

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291928258.png)





#### 解决
```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

#url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312',errors='ignore')
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    t = t.split('【译文】')[1].strip()
    return t
#print(getOne(url))

url = "http://ewenyan.com/contents/more/zgfjs.html"    
def getAll(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312',errors='ignore')
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[2]
    t = t.find_all('a')

    ls = []
    for i in t:
        title = i.text
        link = i.get('href').replace('../../', 'http://ewenyan.com/')
        #print(title, link)
        indx = int(title.split('.')[0])
        ls.append([indx, title, link])
    ls.sort()
    return ls 
        
lst = getAll(url)
 # with语句，自动关闭文件，更安全
with open('曾国藩家书.txt', 'a', encoding='utf-8') as f: 
    for each in lst:
        title = each[1]
        print(title)
        f.write(title)  
        content = getOne(each[2])
        f.write(content)  

```



### 问题二：写入文件时的异常
<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927857.png)





 从网页源码能看到，这一篇其实是有「【译文】」的，大概率是**<font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">网页解析时获取的文本中，「【译文】」前后有多余字符（如空格、换行），或者解析到了广告内容，导致拆分异常</font>**

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927332.png)





#### 添加异常处理
```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

#url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312',errors='ignore')
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    t = t.split('【译文】')[1].strip()
    return t
#print(getOne(url))

url = "http://ewenyan.com/contents/more/zgfjs.html"    
def getAll(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312',errors='ignore')
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[2]
    t = t.find_all('a')

    ls = []
    for i in t:
        title = i.text
        link = i.get('href').replace('../../', 'http://ewenyan.com/')
        #print(title, link)
        indx = int(title.split('.')[0])
        ls.append([indx, title, link])
    ls.sort()
    return ls 
        
lst = getAll(url)
 # with语句，自动关闭文件，更安全
with open('曾国藩家书.txt', 'a', encoding='utf-8') as f: 
    for each in lst:
        try:
            title = each[1]
            print(title)
            f.write(title)  
            content = getOne(each[2])
            f.write(content)
        except:
            print(f'!!!!!!!!!!!第{each[0]}封家书获取内容失败，请处理！！！！！！！！！')
            continue
```

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.jsdelivr.net/gh/XVSHIFU/Picture-bed@img/img/202601291927379.png)





### 完整代码
```python
import requests
from bs4 import BeautifulSoup

myHeader = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

#url = "http://ewenyan.com/articles/zgfjs/1.html"

def getOne(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312',errors='ignore')
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[3].text
    t = t.split('【译文】')[1].strip()
    return t
#print(getOne(url))

url = "http://ewenyan.com/contents/more/zgfjs.html"    
def getAll(url):
    r = requests.get(url, headers = myHeader).content.decode('gb2312',errors='ignore')
    soup = BeautifulSoup(r, 'html.parser')
    t = soup.find_all('table')[2]
    t = t.find_all('a')

    ls = []
    for i in t:
        title = i.text
        link = i.get('href').replace('../../', 'http://ewenyan.com/')
        #print(title, link)
        indx = int(title.split('.')[0])
        ls.append([indx, title, link])
    ls.sort()
    return ls 
        
lst = getAll(url)
for each in lst:
    try:
        with open('曾国藩家书.txt', 'a', encoding='utf-8') as f: 
            title = each[1]
            print(title)
            f.write(title)  
            content = getOne(each[2])
            f.write(content)
    except:
        print(f'!!!!!!!!!!!第{each[0]封家书获取内容失败，请处理！！！！！！！！！')
        continue










        

```























